{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version of Tensorflow\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD= 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3 placeholders - gateways\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define encoder\n",
    "from tensorflow.python.ops.rnn_cell  import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neual Network Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are building a seq2seq Recurrent Neural Network (RNN)) using a bidirectional $encoder$\n",
    "- Simple LSTM Cells are used\n",
    "- Is also possible to use Gated Recurrent Units (GRU's)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units) #20 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Bidirectional RNN Setup\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell, \n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "#When using a standard RNN to make predictions we are only taking the “past” into account. \n",
    "#For certain tasks this makes sense (e.g. predicting the next word), but for some tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenates (tf.concat)\n",
    "\n",
    "Here we keep both $output$ and $state$. We will use the state for attention, which is applied later on. It is possible to drop the states if attention isnt applied and will be more computationally efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#biderectional step\n",
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder contains a simple LSTM cell with attention (soft) - manually defining them. \n",
    "\n",
    "Next we need to decide how far to run decoder. There are several options for stopping criteria:\n",
    "\n",
    "- Stop after specified number of unrolling steps\n",
    "- Stop after model produced\n",
    "\n",
    "Here we are doing a toy copy task, so how about we unroll decoder for len(encoder_input)+2, to allow model some room to make mistakes over 2 additional steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the decoder \n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping of output Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Here we are using a cross-entropy loss function and 'Adam' optimizer algorithm in order to ensure convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[8, 4, 5]\n",
      "[3, 4, 9, 7, 2]\n",
      "[9, 9, 3, 9]\n",
      "[5, 3, 5, 3, 4]\n",
      "[2, 4, 9]\n",
      "[7, 2, 6]\n",
      "[4, 4, 2, 2]\n",
      "[2, 4, 3, 9, 9, 5, 2]\n",
      "[6, 4, 9, 3, 5, 6, 9]\n",
      "[9, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 0.13415655493736267\n",
      "  sample 1:\n",
      "    input     > [4 7 9 8 2 6 0 0]\n",
      "    predicted > [4 7 9 8 2 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 4 3 0 0 0 0 0]\n",
      "    predicted > [3 4 3 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 9 8 7 8 0 0 0]\n",
      "    predicted > [7 9 8 7 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.10308825969696045\n",
      "  sample 1:\n",
      "    input     > [6 8 9 0 0 0 0 0]\n",
      "    predicted > [6 8 9 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 7 8 3 7 0 0 0]\n",
      "    predicted > [3 7 8 3 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 2 6 2 5 6 3 6]\n",
      "    predicted > [5 2 6 6 5 6 3 6 1 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.05600270628929138\n",
      "  sample 1:\n",
      "    input     > [8 2 7 0 0 0 0 0]\n",
      "    predicted > [8 2 7 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 6 8 6 2 8 0]\n",
      "    predicted > [6 7 6 8 6 2 8 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 8 4 2 8 6 0]\n",
      "    predicted > [6 6 8 4 2 8 6 1 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.04189540445804596\n",
      "  sample 1:\n",
      "    input     > [7 5 6 9 8 9 5 0]\n",
      "    predicted > [7 5 6 9 8 9 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 8 3 4 0 0 0 0]\n",
      "    predicted > [2 8 3 4 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 4 9 2 5 3 0]\n",
      "    predicted > [9 9 4 9 2 5 3 1 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1446 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESkiCvoiIkVEQV9EpIgo6IuIFJH/B9TBPDrlvNryAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x35bdc828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
